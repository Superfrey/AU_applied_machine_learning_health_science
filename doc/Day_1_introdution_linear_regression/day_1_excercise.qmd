
```{r, include=FALSE}
library("here")
```

## Excercise Day 1

### Introduction

**Check point 1: Supervised learning example**

*Example 1* Research questions: Could we predict HbA1c given various measurement of cholesterol, bmi, age, blood pressure, and sex.

Method: Linear regression

Data set: tabular data set with age, sex, body measures and biochemical measures.

*Example 2*

**Check point 2:**

Find part from ECG nodes in ECG data to detect early high risk of CVD event. Mixed inference and prediction. As ECG have large variability, I believe to include many recordings (\> 1000).

Methods: Deep learning neural network ?

Data: Time to event data - ECG data for each individuals - Hospital records of CVD events

Checkpoint 4: Ï) Linear regression - body density data
In this checkpoint you will analyse the body density data using linear regression.
The goal is to build a linear regression model to predict body density from chest
circumference measurements.
• Open the script main1a.m and review it to understand the different steps in the
script. Some of the code needed to answer this checkpoint is already provided
in the script, but you also need to do a bit of coding yourself.

Use the script main1a.m / main1a.R and your code to answer the following:

(a) Load the data set bodyMeasurementsSingleTrainTest.txt. Describe the data,
what are the sizes of the training- and test sets? how many observations and
features? what is the numerical range of the variables?

test = 244 obs with 2 variables
train = 8 obs with 2 variables


(b) Identify the part for of the script that is used for creating the polynomial expansion
of the input variable. Try do create polynomial expansions with different
polynomial order from 1 to 7, how many columns/features are there in the resulting
data matrices?

n+1 for each polynomial increase (1 polynum = 1 features and 2 polynum = 2 features)


(c) Part of the code is scaling the individual columns of the polynomial regressors.
Explain how this scaling is done and why the scaling may be necessary (Hint:
what is the numerical range of the individual columns?).



(d) Type doc fitglm in Matlab or ?glm in R and use a bit of time to familiarize
yourself with this function. What are the inputs to the function and what is the
output?

GLM is generalised linear model, and can be used for a variety of regression based defined familiy. The gaussian is used for linear regression with option for polynomial flexibility.
The inputs are continuous and the output is a predicted continuous response.

(e) Explain how training error and test error are quantified in the script?

Training error is the mean of the squared difference between errors, based on the difference between observed value from the training data and predicted value from the training data set.

Test error is the mean of the squared difference between errors, based on the difference between observed value from the test data set and predicted value from the training dataset.

(f) Run the analysis with polynomial order ranging from 1 to 7 (Hint: include a
for-loop in the script). For each of these seven models, make a plot of body
density (ordinate) vs. chest circumference (abscissa) for the training- and test
data as well as the model’s predictions (all three in the same plot). Include the
plots in your report and describe/discuss the plots.

```{r, include=FALSE}
source(here("R/day_1_code/main1a.R"))
```





(g) Write your own code to make a plot of training error and test error vs. polynomial
order (order 1 to 7). Include the plot in your report and describe/discuss
the plot. Do you observe severe overfitting for some polynomial orders?



```{r}
plot(MSE_error_plot)
head(MSE_poly,7)
```

Checkpoint 5: b
Training- and test errors

(a) Explain the difference between a training- and a test data. Also explain
the difference between training- and test error.

The training data is used for training the models for predicting a certain outcome. The test data is used to test the trained model, to evaluate how well the trained model are predicting. The tools for evaluate the models performance is based on training and testing error. The training error are the error from the devolped model on the trained data point, where it is the same for testing, just focusing on the testing data points.

(b) Argue why we typically are interested in good model performance in terms of low test error rather than in terms of low training error.

We like to prioritize low error test results because the fitted model true evaluation depends how it fit with the validation points from test data set.


**Checkpoint 6: b Do section 2.4 conceptual exercise 3. in ISL (ISL page 53).**

3. We now revisit the bias-variance decomposition.
(a) Provide a sketch of typical (squared) bias, variance, training error, test error, and Bayes (or irreducible) error curves, on a single plot, as we go from less flexible statistical learning methods towards more flexible approaches. The x-axis should represent the amount of flexibility in the method, and the y-axis should represent the values for each curve. There should be five curves. Make sure to label each one.


```{r, echo=FALSE}
plot(NULL, xlim=c(0,1), ylim=c(0,1), xlab="Flexibility", ylab="Error")

# Generate fake data for the curves
x <- seq(0, 1, length.out=100)
bias <- x^2
variance <- (1 - x)^2
training_error <- bias + variance
test_error <- training_error + 0.1*x
bayes <- 0.25*x

# Add the curves to the plot
lines(x, bias, col="red")
lines(x, variance, col="blue")
lines(x, training_error, col="green")
lines(x, test_error, col="orange")
lines(x, bayes, col="purple")

# Add a legend
legend("topright", c("Bias", "Variance", "Training Error", "Test Error", "Bayes Error"), col=c("red", "blue", "green", "orange", "purple"), lty=1)

# Annotate the plot to show the expected behavior of the training error and test error curves
arrows(x0=0.7, y0=0.6, x1=0.9, y1=0.8, length=0.1, angle=90, code=3)
text(x=0.8, y=0.7, "Overfitting", cex=0.8)
arrows(x0=0.2, y0=0.3, x1=0.0, y1=0.1, length=0.1, angle=90, code=3)
text(x=0.1, y=0.2, "Underfitting", cex=0.8)
```

**(b) Explain why each of the five curves has the shape displayed in part (a).**

As training error decreases because of better fitting, howvere over fitting is introduced and causes increase in test error. Bias decrease in training data with high complexity. However, the trade off is that variance in the test are increasing. So we want to chose the flexibility point were bias and variance in combination are lowest.

Checkpoint 7:

Explain, in your own words, what a training set is, a validation set is, and a
test set is? Why is this partition of data needed? Explain how k-fold cross validation
is implemented. Explain how leave-one-out (LOO) cross validation is implemented.
What are the advantages and disadvantages of k-fold cross validation relative to the
validation set approach and the LOO cross validation approach?

Training set is data that is used for traing the prediction model, where validation is used for evaluated how well the model predicts the outcoem in interest. 

Checkpoint 8: Ï

R users: By using the functions createDataPartition and createFolds we can
create random partitions of data.

• Look at the help text for these functions and use a bit of time to familiarize
yourself with the functions.

?

(a) Run the command c = createDataPartition(y = 1:20, p = 0.8) and explain
what the function does. Also describe the content of the variable c.

**It creates a list of numbers (series) of partition from 1 - 20**

(b) Run the command c = createFolds(y = 1:20, k = 10, returnTrain =
TRUE) and explain what the function does. Also explain the content of the
variable c.

CreateFold split up the data in k groups. Here c are devided into 10 k-folds with 20 vector outcomes.

(c) Explain why it is generally recommended to run e.g. the command set.seed(0)
before creating the random partitions.

I dont know... To set up the seed before deviding the dataset, hence obtatin reproduceability.




